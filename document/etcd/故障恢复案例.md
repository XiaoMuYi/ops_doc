# ETCD 集群故障恢复案例

## 场景说明

关于`etcd`故障情况，一般分为主机节点故障和误删除数据这两种情况。那么对于数据删除这种情况，从备份快照数据中恢复数据是相当简单的，这里就不做过多讲解。

在实际生产环境会经常遇到服务器宕机，造成服务器不可恢复或发生主机`IP`变更（事实证明云环境确实会发生`IP`变化且不可设置静态`IP`）。那么在这种情况该如何恢复集群可用状态呢？本篇文章不会过多讲解关于`etcd`高可用集群的实现以及理论，本篇文章会按照生产环境中的3节点集群为例进行故障恢复。

## 故障案例1，某节点宕机发生IP地址变更

这里我手动将`etcd1`主机的`ip`从`192.168.133.128`变更为`192.168.133.131`，来模拟在云环境下由于主机故障重启发生的`IP`地址变更。

### 1.1 检查集群状态

```shell
$ ETCDCTL_API=3 etcdctl \
    --cacert=/etc/kubernetes/ssl/ca.pem \
    --cert=/etc/etcd/ssl/etcd.pem \
    --key=/etc/etcd/ssl/etcd-key.pem \
    --endpoints=https://192.168.133.128:2379,https://192.168.133.129:2379,https://192.168.133.130:2379 \
    endpoint health
```

此时通过执行上面的命令，可以看到提示说`https://192.168.133.128:2379 is unhealthy: failed to connect: context deadline exceeded`。

### 1.2 重新生成证书以及证书文件

将新的`IP`地址`192.168.133.131`加入到证书中，证书生成请[参考链接](https://github.com/XiaoMuYi/k8s-deployment-cluster/blob/master/02.%E5%88%9B%E5%BB%BAetcd%E9%9B%86%E7%BE%A4.md)。证书生成之后通过`scp`命令拷贝到其他主机。

```shell
scp /etc/etcd/ssl/* 192.168.133.129:/etc/etcd/ssl/
scp /etc/etcd/ssl/* 192.168.133.130:/etc/etcd/ssl/
```

更改服务启动文件内容，将`--initial-cluster`中的地址从`192.168.133.128`改为`192.168.133.131`，然后重启各节点的`etcd`服务。

### 1.3 重启服务并验证集群状态

```shell
systemctl damemon-reload
systemctl restart etcd
```

重启服务之后，可以看到当前`etcd`集群信息如下：

```shell
$ ETCDCTL_API=3 etcdctl \
    --cacert=/etc/kubernetes/ssl/ca.pem \
    --cert=/etc/etcd/ssl/etcd.pem \
    --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.133.128:2379,https://192.168.133.129:2379,https://192.168.133.130:2379 \
    member list
97df7be541e14e1, started, etcd1, https://192.168.133.128:2380, https://192.168.133.131:2379
4a03f8e9dcfbff82, started, etcd3, https://192.168.133.130:2380, https://192.168.133.130:2379
65fd21291c548d00, started, etcd2, https://192.168.133.129:2380, https://192.168.133.129:2379
```

可以看到`etcd1`存在两个地址`https://192.168.133.128:2380, https://192.168.133.131:2379`，因此还需要更新`etcd1`的地址信息。

```shell
$ ETCDCTL_API=3 etcdctl \
    --cacert=/etc/kubernetes/ssl/ca.pem \
    --cert=/etc/etcd/ssl/etcd.pem \
    --key=/etc/etcd/ssl/etcd-key.pem \
    --endpoints=https://192.168.133.131:2379,https://192.168.133.129:2379,https://192.168.133.130:2379 \
    member update 97df7be541e14e1 \
    --peer-urls=https://192.168.133.131:2380
Member  97df7be541e14e1 updated in cluster 442ff048058f1927
```

再次查看集群状态

```shell
$ ETCDCTL_API=3 etcdctl  \
   --cacert=/etc/kubernetes/ssl/ca.pem \
   --cert=/etc/etcd/ssl/etcd.pem \
   --key=/etc/etcd/ssl/etcd-key.pem \
   --endpoints=https://192.168.133.128:2379,https://192.168.133.129:2379,https://192.168.133.130:2379 \
   member list
97df7be541e14e1, started, etcd1, https://192.168.133.131:2380, https://192.168.133.131:2379
4a03f8e9dcfbff82, started, etcd3, https://192.168.133.130:2380, https://192.168.133.130:2379
65fd21291c548d00, started, etcd2, https://192.168.133.129:2380, https://192.168.133.129:237
```

成功恢复！

## 故障案例2，某主机宕机不可恢复

这里我手动将`etcd1`主机`192.168.133.128`直接关机，来模拟服务器宕机并且无法开机。因此需要重新开启一台虚拟机，地址为`192.168.133.131`。

### 2.1 安装新的节点

```shell
ansible-playbook -i inventory/hosts.multi-master 02.etcd.yml --limit=192.168.133.131
```

通过`ansible playbook`实现新节点的安装操作，会重新生成新的证书。这里需要将安装的`etcd`服务停止，并且清理掉`/var/lib/etcd`目录下的数据，然后拷贝该主机的证书文件到其他主机。

```shell
# 拷贝新的证书文件到其他主机
$ scp /etc/etcd/ssl/* 192.168.133.129:/etc/etcd/ssl/
$ scp /etc/etcd/ssl/* 192.168.133.130:/etc/etcd/ssl/

# 停止服务
$ systemctl stop etcd
$ rm -rf /var/lib/etcd/member
```

### 2.2 备份数据并恢复一份宕机主机的数据

```shell
# 在运行正常的主机进行备份
$ ETCDCTL_API=3 etcdctl snapshot save snapshot.db

# 检查备份状态
$ ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db
```

恢复出原故障主机`192.168.133.128`的数据；

```shell
$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
--name etcd1 \
--initial-cluster etcd1=https://192.168.133.128:2380,etcd2=https://192.168.133.129:2380,etcd3=https://192.168.133.130:2380 \
--initial-cluster-token etcd-cluster-0 \
--initial-advertise-peer-urls https://192.168.133.128:2380
```

拷贝数据到`192.168.133.128`主机；

```shell
scp -r etcd1.etcd/member 192.168.133.131:/var/lib/etcd/
```

更改服务启动文件内容，将`--initial-cluster`中的地址从`192.168.133.128`改为`192.168.133.131`，然后重启各节点的`etcd`服务。

### 2.3 重启服务并验证集群状态

```shell
systemctl daemon-reload
systemctl restart etcd
```

可以看到`etcd1`存在两个地址`https://192.168.133.128:2380, https://192.168.133.131:2379`，因此还需要更新`etcd1`的地址信息。

```shell
$ ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/ssl/ca.pem \
   --cert=/etc/etcd/ssl/etcd.pem \
   --key=/etc/etcd/ssl/etcd-key.pem \
   --endpoints=https://192.168.133.131:2379,https://192.168.133.129:2379,https://192.168.133.130:2379 \
   member update 97df7be541e14e1 --peer-urls=https://192.168.133.131:2380
```

再次查看集群状态

```shell
$ ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/ssl/ca.pem \
   --cert=/etc/etcd/ssl/etcd.pem \
   --key=/etc/etcd/ssl/etcd-key.pem \
   --endpoints=https://192.168.133.128:2379,https://192.168.133.1:2379,https://192.168.133.130:2379  \
   member list
97df7be541e14e1, started, etcd1, https://192.168.133.131:2380, https://192.168.133.131:2379
4a03f8e9dcfbff82, started, etcd3, https://192.168.133.130:2380, https://192.168.133.130:2379
65fd21291c548d00, started, etcd2, https://192.168.133.129:2380, https://192.168.133.129:237
```

## 故障案例3，机房宕机坏掉2个节点

由于机房断电，测试环境`ETCD`，三个节点坏掉了两个。本来想通过备份恢复，可是发现由于是突然宕机没有跑备份脚本，因此三个节点自杀了两个节点，就一个节点无法形成集群。
于是进行了以下操作。

### 3.1 停止kubernetes api-service

```shell
systemctl stop kube-apiserver
```

### 3.2 修改etcd配置文件

```shell
# 首先对各节点数据进行备份
$ cp -r /var/lib/etcd/etcd.etcd /var/lib/etcd/etcd.etcd.bak
```

由于集群不能通过正常的方式恢复，因此需要将活着的`ETCD`使用但节点集群模式运行，比如我这里存活节点为`192.168.133.130`。因此，我这里需要修改`/etc/systemd/system/etcd.service`文件，修改内容为`ETCD_INITIAL_CLUSTER="etcd2=https://192.168.133.130:2380"`。并且在启动命令中添`--force-new-cluster`。

然后重启etcd

```shell
systemctl daemon-reload
systemctl restart etcd
```

### 3.3 添加新的成员（活着的节点上操作）

```shell
$ export ETCDCTL_API=3
$ etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem \
--cert=/etc/etcd/ssl/etcd.pem \
--key=/etc/etcd/ssl/etcd-key.pem \
--endpoints=https://192.168.133.129:2379 \
member add etcd2 --peer-urls=https://192.168.133.129:2380

$ etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem \
--cert=/etc/etcd/ssl/etcd.pem \
--key=/etc/etcd/ssl/etcd-key.pem \
--endpoints=https://192.168.133.131:2379 \
member add etcd2 --peer-urls=https://192.168.133.131:2380
```

删除宕掉的节点上面的残留数据（死掉的节点上面操作）

```shell
rm -rf /var/lib/etcd/etcd.etcd
rm -rf /var/lib/etcd/wal
```

修改`/etc/systemd/system/etcd.service`文件中的内容启动参数，将`ETCD_INITIAL_CLUSTER_STATE`中的`new`改为`existing`。

启动dang掉的节点。

```shell
systemctl start etcd
```

两台宕掉的节点上面操作完成后，etcd集群已经启动。检查下健康状态：

```shell
$ export ETCDCTL_API=3
$ etcdctl --cacert=/etc/etcd/ssl/etcd-root-ca.pem \
--cert=/etc/etcd/ssl/etcd.pem \
--key=/etc/etcd/ssl/etcd-key.pem \
--endpoints=https://192.168.133.129:2379,https://192.168.133.130:2379,https://192.168.133.131:2379 \
endpoint health
```

发现集群已经恢复了。最后去掉原本存活的etcd机器上面--force-new-cluster命令。将节点改成3个地址，然后重启etcd，启动kube-apiserver。
