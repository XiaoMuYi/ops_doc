# Ceph 使用手册

## 1. 安装ceph

目前`ceph`提供对象存储`RADOSGW`、块存储`RDB`以及`CephFS`文件系统这 3 种功能。对于这3种功能介绍，分别如下：

* 对象存储，也就是通常意义的键值存储，其接口就是简单的`GET`、`PUT`、`DEL`和其他扩展，代表主要有`Swift`、`S3`以及`gluster`等；
* 块存储，这种接口通常以`QEMU Driver`或者`Kernel Module`的方式存在，这种接口需要实现`Linux`的`Block Device`的接口或者由`QEMU`提供的`Block Driver`接口，如`Sheepdog`，`AWS`的`EBS`，青云的云硬盘和阿里云的盘古系统，还有`Ceph`的`RBD`（`RBD`是`Ceph`面向块存储的接口）。在常见的存储中`DAS`，`SAN`提供的也是块存储；
* 文件存储，通常意义是支持`POSIX`接口，它跟传统的文件系统如`Ext4`是一个类型的，但区别在于分布式存储提供了并行化的能力，如`Ceph`的`CephFS` (`CephFS`是`Ceph`面向文件存储的接口)，但是有时候又会把`GlusterFS` ，`HDFS`这种非`POSIX`接口的类文件存储接口归入此类。当然`NFS`、`NAS`也是属于文件系统存储。

### 1.1 安装ansible

```shell
yum -y install http://dist.yongche.com/centos/7/epel/x86_64/Packages/a/ansible-2.6.5-1.el7.noarch.rpm
```
提示：官方对`ansible`版本有明确的要求，目前支持 2.4 以及 2.6 不能太高也不能太新。

### 1.2 下载 ceph-ansible
到 https://github.com/ceph/ceph-ansible/releases 下载最新稳定版本，建议下载对应自己想安装的版本。如何知道自己下载的 `ceph-ansible`支持自己想安装的`ceph`版本？只能查看与之对应的 `Changelog`。我这里环境的版本`v3.2.0`。

### 1.3 添加hosts文件
```shell
$ cd /home/yangsheng/ceph-ansible-3.2.0
$ cat ./hosts
[mons]
172.17.3.32
172.17.3.33
172.17.3.34

[osds]
172.17.3.32
172.17.3.33
172.17.3.34

[mgrs]
172.17.3.32
172.17.3.33
172.17.3.34

[mdss]
172.17.3.32
172.17.3.33
172.17.3.34

[clients]
172.17.80.29
172.17.80.30
172.17.80.31

$ cp group_vars/all.yml.sample group_vars/all.yml
$ cp group_vars/osds.yml.sample group_vars/osds.yml
$ cp site.yml.sample site.yml
```

### 1.4 配置全局变量
```shell
$ egrep -v "^#|^$" group_vars/all.yml
---
dummy:
ceph_origin: repository
ceph_repository: community
ceph_mirror: https://mirrors.aliyun.com/ceph
ceph_stable_key: https://mirrors.aliyun.com/ceph/keys/release.asc
ceph_stable_release: mimic
ceph_stable_repo: "{{ ceph_mirror }}/rpm-{{ ceph_stable_release }}"
fsid: 17ffc828-5d8c-4937-a5bb-f6adb2384d20
generate_fsid: true
ceph_conf_key_directory: /etc/ceph
cephx: true
monitor_interface: bond0
public_network: 172.17.0.0/16
cluster_network: 172.17.0.0/16
ceph_conf_overrides:
  global:
    rbd_default_features: 7
    auth cluster required: cephx
    auth service required: cephx
    auth client required: cephx
    osd journal size: 2048
    osd pool default size: 3
    osd pool default min size: 1
    mon_pg_warn_max_per_osd: 1024
    osd pool default pg num: 1024
    osd pool default pgp num: 1024
    max open files: 131072
    osd_deep_scrub_randomize_ratio: 0.01
  mon:
    mon_allow_pool_delete: true

  client:
    rbd_cache: true
    rbd_cache_size: 335544320
    rbd_cache_max_dirty: 134217728
    rbd_cache_max_dirty_age: 10

  osd:
    osd mkfs type: xfs
    ms_bind_port_max: 7100
    osd_client_message_size_cap: 2147483648
    osd_crush_update_on_start: true
    osd_deep_scrub_stride: 131072
    osd_disk_threads: 4
    osd_map_cache_bl_size: 128
    osd_max_object_name_len: 256
    osd_max_object_namespace_len: 64
    osd_max_write_size: 1024
    osd_op_threads: 8
    osd_recovery_op_priority: 1
    osd_recovery_max_active: 1
    osd_recovery_max_single_start: 1
    osd_recovery_max_chunk: 1048576
    osd_recovery_threads: 1
    osd_max_backfills: 4
    osd_scrub_begin_hour: 23
    osd_scrub_end_hour: 7

$ egrep -v "^#|^$" group_vars/mgrs.yml
---
dummy:
ceph_mgr_modules: [status,dashboard]
```

### 1.5 osds.yml 内容如下
```shell
$ egrep -v "^#|^$" group_vars/osds.yml
---
dummy:
devices:
  - /dev/sdb
  - /dev/sdc
  - /dev/sdd
  - /dev/sde
  - /dev/sdf
  - /dev/sdg
osd_scenario: collocated
osd_objectstore: bluestore
```

### 1.6 site.yml 内容如下
```shell
$ egrep -v "^#|^$" site.yml
---
- hosts:
  - mons
  - osds
  - mdss
  - clients
  - mgrs
```
提示：这里只需要注释掉其他内容即可，我这里显示的是注释后的内容。

1.7 执行安装操作
```shell
$ ansible-playbook -i hosts site.yml

# 重启服务
systemctl restart ceph-mds.target
systemctl restart ceph-mgr.target
systemctl restart ceph-mon.target
systemctl restart ceph-osd.target
```

## 2. cephFS
`cephfs`，需要部署`mds`（元数据服务器）。基本依赖解决之后，就可以为`cephfs`创建`pool`，并且至少需要两个`rados`池，一个用于数据，一个用于元数据。我们这里是`ansible`部署，所以很多过程已经实现。手动操作如下：

### 2.1 创建pool
```shell
$ ceph osd pool create cephfs_data 128
pool 'cephfs_data' created

$ ceph osd pool create cephfs_metadata 128
pool 'cephfs_metadata' created
```

### 2.2 创建文件系统
```shell
$ ceph fs new cephfs cephfs_metadata cephfs_data
$ ceph fs ls        # 查看创建好的cephFS
```


### 2.3 遇到的问题总结

通过命令`ceph health`可以看到如下提示说每个`osd`上的`pg`数量小于最小的数目 30 个。`pgs`为 16，因为是 3 副本的配置，所以当有 18 个`osd`的时候，每个`osd`上均分了 `16/18 *3=2`个`pgs`,也就是出现了如上的错误“小于最小配置30个”。

```shell
$ ceph health
HEALTH_WARN too few PGs per OSD (2 < min 30)

$ ceph -s
  cluster:
    id:     7e2de501-7b34-4121-a431-0776ee1cb004
    health: HEALTH_WARN
            too few PGs per OSD (2 < min 30)

  services:
    mon: 3 daemons, quorum k8store01,k8store02,k8store03
    mgr: k8store02(active), standbys: k8store01, k8store03
    mds: cephfs-1/1/1 up  {0=k8store03=up:active}, 2 up:standby
    osd: 18 osds: 18 up, 18 in

  data:
    pools:   2 pools, 16 pgs
    objects: 22  objects, 2.2 KiB
    usage:   18 GiB used, 33 TiB / 33 TiB avail
    pgs:     16 active+clean
```

集群这种状态下如果进行数据的存储和操作，会引发集群卡死，无法响应`io`，同时会导致大面积的`osd down`。

`cephfs`需要用到两个`pool` : `fs_data`和`fs_metadata`。在初次使用`ceph`之前需要首先规划：集群一共承载多少存储业务，创建多少个 pool，最后得到每个存储应该分配多少个pg。

参考链接：http://docs.ceph.com/docs/mimic/rados/operations/placement-groups/

必须选择pg_num的值，因为它无法自动计算。以下是常用的一些值：
* 少于 5 `OSDs` ，pg_num 设置为 128；
* 在 5 到 10 `OSDs` 之间，pg_num 设置为 512；
* 在 10 到 50 `OSDs`之间，pg_num 设置为 1024；

如果您有超过50个`OSD`，则需要了解折衷以及如何自己计算`pg_num`值要自己计算`pg_num`值，请使用`pgcalc`工具。

参考链接：http://docs.ceph.com/docs/mimic/rados/operations/placement-groups/

关于`cephfs_metadata pug_num`正确配置参考：https://ceph.com/planet/cephfs-ideal-pg-ratio-between-metadata-and-data-pools/

查看当前pg数
```shell
$ ceph osd pool get cephfs_data pg_num
pg_num: 8
$ ceph osd pool get cephfs_data pgp_num
pgp_num: 8

$ ceph osd pool get cephfs_metadata pg_num
pg_num: 8
$ ceph osd pool get cephfs_metadata pgp_num
pgp_num: 8


$ ceph osd pool set cephfs_data pg_num 1024
Error E2BIG: specified pg_num 1024 is too large (creating 1016 new PGs on ~8 OSDs exceeds per-OSD max with mon_osd_max_split_count of 32)
```
结果出现这个错误，参考 http://www.selinuxplus.com/?p=782， 原来是一次增加的数量有限制。最后选择用暴力的方法解决问题：

设置 cephfs_metadata pg_num 数
```shell
$ ceph osd pool set cephfs_metadata pg_num 32
$ ceph osd pool set cephfs_metadata pgp_num 32

$ ceph osd pool set cephfs_metadata pg_num 64
$ ceph osd pool set cephfs_metadata pgp_num 64

$ ceph osd pool set cephfs_metadata pg_num 128
$ ceph osd pool set cephfs_metadata pgp_num 128
```

设置 cephfs_metadata pg_num 数
```shell
$ ceph osd pool set cephfs_data pg_num 128
$ ceph osd pool set cephfs_data pgp_num 128

$ ceph osd pool set cephfs_data pg_num 512
$ ceph osd pool set cephfs_adata pgp_num 512

$ ceph osd pool set cephfs_data pg_num 1024
$ ceph osd pool set cephfs_data pgp_num 1024
```

### 2.4 kubernetes集成cephfs案例

创建`ceph-secret`这个`k8s secret`对象，在`ceph`集群主机执行。
```shell
$ ceph auth get-key client.admin
AQCCVSBcLK5nLhAAD3sehi8lweCwT+FJbvGSIA==
```

在`kubernetes master`主机执行
```shell
$ echo "AQCCVSBcLK5nLhAAD3sehi8lweCwT+FJbvGSIA==" > /tmp/secret
$ kubectl create ns cephfs
$ kubectl create secret generic ceph-secret-admin --from-file=/tmp/secret --namespace=cephfs
```

部署`CephFS provisioner Install with RBAC roles`
```shell
$ git clone https://github.com/kubernetes-incubator/external-storage.git
$ cd external-storage/ceph/cephfs/deploy
$ NAMESPACE=cephfs
$ sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/*.yaml
$ kubectl -n $NAMESPACE apply -f ./rbac
```

参考链接：https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs/deploy

创建一个`storageclass`
```shell
$ cat local-class.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: cephfs
  namespace: cephfs
provisioner: ceph.com/cephfs
parameters:
    monitors: 172.17.3.32:6789,172.17.3.33:6789,172.17.3.34:6789
    adminId: admin
    adminSecretName: ceph-secret-admin
    adminSecretNamespace: "cephfs"


$ kubectl create -f local-class.yaml
$ kubectl get storageclass
NAME     PROVISIONER       AGE
cephfs   ceph.com/cephfs   10s
```

创建`PVC`使用`cephfs storageClass`动态分配`PV`
```shell
$ cat local-claim.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-local
  namespace:  cephfs
  annotations:
    volume.beta.kubernetes.io/storage-class: "cephfs"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

$ kubectl create -f local-claim.yaml
$ kubectl get pvc claim-local -n cephfs
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-local   Bound    pvc-127dd47b-08ee-11e9-9e4f-faf206331800   1Gi        RWX            cephfs         8s
```

创建一个`Pod`绑定该`PVC`

```shell
$ cat test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: cephfs-pvc-pod
  name: cephfs-pv-pod1
  namespace: cephfs
spec:
  containers:
  - name: cephfs-pv-busybox1
    image: busybox
    command: ["sleep", "60000"]
    volumeMounts:
    - mountPath: "/mnt/cephfs"
      name: cephfs-vol1
      readOnly: false
  volumes:
  - name: cephfs-vol1
    persistentVolumeClaim:
      claimName: claim-local
```

创建`Pod`，并检查是否挂载`cephfs`卷成功

```shell
$ kubectl create -f test-pod.yaml -n cephfs
$ kubectl get pod cephfs-pv-pod1 -n cephfs
```

提示：`secret`和`provisioner`不在同一个`namespace`中的话，获取`secret`权限不够。

## 3. 集群运维相关

### 3.1 查看集群状态
```shell
$ ceph -s        # 查看集群状态
$ ceph health    # 查看集群监控状态
$ ceph -w        # 观察集群健康状况
```

### 3.2 查看ceph存储空间
```shell
$ ceph df        # 查看集群存储空间
$ ceph mds stat  # 查看 mds 状态
```

### 3.3 查看mon相关信息
```shell
$ ceph mon stat         # 查看mon状态信息
$ ceph quorum_status    # 查看mon选举状态
$ ceph mon dump         # 查看mon映射信息
$ ceph daemon k8store01  mon_status        # 查看mon详细状态
$ ceph quorum_status --format json-pretty  # 查看mon仲裁状态
```

### 3.4 查看osd相关信息
```shell
$ ceph osd stat         # 查看osd运行状态
$ ceph osd dump         # 查看osd映射信息
$ ceph osd perf         # 查看数据延迟
$ ceph osd df           # 详细列出集群每块磁盘的使用情况  
$ ceph osd tree         # 查看osd目录树
$ ceph osd getmaxosd    # 查看最大osd的个数
$ ceph osd lspools      # 列出ceph存储池
```

### 3.5 查看PG信息
```
$ ceph pg dump       	# 查看 PG 组的映射信息
$ ceph pg stat       	# 查看 PG 状态
$ ceph pg dump --format plain        		# 显示集群中的所有的 PG 统计,可用格式有纯文本plain(默认)和json
```

### 3.6 启用dashboard
```
$ ceph mgr module enable dashboard	# 启用dashboard模块
```

默认情况下，仪表板的所有`HTTP`连接均使用`SSL/TLS`进行保护。要快速启动并运行仪表板，可以使用以下内置命令生成并安装自签名证书:
```
$ ceph dashboard create-self-signed-cert
```

创建具有管理员角色的用户
```
$ ceph dashboard set-login-credentials admin admin
```

默认下，仪表板的守护程序(即当前活动的管理器)将绑定到`TCP`端口`8443`或`8080`。
```
$ ceph mgr services
{
    "dashboard": "https://k8store01.ops.bj2.yongche.com:8443/"
}

$ ceph config-key set mgr/dashboard/server_addr 0.0.0.0
$ ceph config-key set mgr/dashboard/server_port 9000
$ systemctl restart ceph-mgr\@k8store01.service
```
