# ceph理论以及常见面试问题

## 1. ceph架构概述

* `ceph monitor`，简称`MON`，负载维护整个集群的运行健康状况监控，信息由集群成员的守护程序来提供，`ceph monitor map`主要包括`osd map`、`pg map`、`mds map`、`crush`等。

* `ceph对象存储设备（OSD）`，由物理磁盘驱动器在其`Linux`文件系统以及`ceph osd`服务组成。`osd`将数据以对象的形式存储到集群中的每个节点的物理磁盘上，完成存储数据的工作绝大多数是由`osd daemon`进程实现。`ceph`集群中的物理磁盘的总数，与磁盘上运维的存储用户数据的`osd`守护进程的熟练是相同的。

* `MDS`，只为`cephfs`跟踪文件的层次结构和存储元数据。`ceph`块设备和`rados`不需要`mds`，`mds`只为 `cephfs`服务。

* `RADOS`，全称`Reliable Autonomic Distributed Object Store`，即可靠分布式对象存储。在`ceph`中所有数据都以对象形式存储，`RADOS`对象存储都将负责保存这些对象。`RADOS`层可以确保数据始终保持一致，不过需要执行数据复制、故障检测和恢复，以及数据迁移和在所有节点实现再平衡。

* `librados`，`librados`库为应用程序提供访问接口，同事也为块存储、对象存储、文件系统原生的接口。

* `RADOS`块设备(`RBD`)，它能够自动精简配置并可能调整大小，而且将数据分散存储在多个`osd`上。

* `RADOS`网关接口（RGW），提供对象存储服务。它使用`librgw`和 `librados`来实现允许应用程序与`ceph`对象存储建立连接。并且提供`s3`和`swift`兼容的`RESTful API`接口。

* `CephFS`，`ceph`文件系统，提供了一个使用`ceph`存储集群用户数据的与`POSIX`兼容的文件系统。`Ceph`文件系统至少需要两个`RADOS`池，一个用于数据，一个用于元数据。

* `CRUSH`，全称`Controlled Replication Under Scalable Hashing`，它表示数据存储的分布式选择算法，`CRUSH`算法取代了在元数据表中为每个客户端请求进行查找，它通过计算机系统中数据应该被写入或读出的位置。`CRUSH`能够感知基础架构，能够理解基础设施各个部件之间的关系。并且`CRUSH`保存数据的多个副本，这样即使一个故障域的几个组件都出现故障，数据依然可用。`CRUSH`算是使得`ceph`实现了自我管理和自我修复。

